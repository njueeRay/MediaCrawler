#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–è°ƒåº¦å™¨ - MediaCrawler-é£ä¹¦åŒæ­¥
æ”¯æŒå®šæ—¶ä»»åŠ¡ã€æ–‡ä»¶ç›‘æ§ã€AIæ•°æ®æ¸…æ´—ç­‰è‡ªåŠ¨åŒ–æµç¨‹
"""

import os
import time
import schedule
import logging
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# å¯¼å…¥ç°æœ‰æ¨¡å—
from feishu_sync_simple import FeishuSimpleSync
from feishu_sync.config import load_config_from_env
from feishu_sync.data_formatter import XHSDataFormatter

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scheduler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class DataFileHandler(FileSystemEventHandler):
    """æ–‡ä»¶ç›‘æ§å¤„ç†å™¨"""
    
    def __init__(self, sync_manager):
        self.sync_manager = sync_manager
        
    def on_created(self, event):
        if not event.is_directory and event.src_path.endswith('.json'):
            logger.info(f"ğŸ” æ£€æµ‹åˆ°æ–°æ–‡ä»¶: {event.src_path}")
            # å»¶è¿Ÿå¤„ç†ï¼Œç¡®ä¿æ–‡ä»¶å†™å…¥å®Œæˆ
            time.sleep(2)
            self.sync_manager.process_new_file(event.src_path)

class AutoScheduler:
    """è‡ªåŠ¨åŒ–è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.config = load_config_from_env()
        self.sync = FeishuSimpleSync(
            app_id=self.config['app_id'],
            app_secret=self.config['app_secret'],
            app_token=self.config['app_token']
        )
        self.watch_dir = "data/xhs/json"
        self.observer = None
        
    def setup_scheduled_tasks(self):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
        
        # æ¯æ—¥2ç‚¹æ‰§è¡Œå…¨é‡åŒæ­¥
        schedule.every().day.at("02:00").do(self.daily_sync_task)
        
        # æ¯å°æ—¶æ£€æŸ¥æ–°æ•°æ®
        schedule.every().hour.do(self.hourly_check_task)
        
        # æ¯6å°æ—¶æ¸…ç†æ—¥å¿—
        schedule.every(6).hours.do(self.cleanup_logs_task)
        
        logger.info("ğŸ“… å®šæ—¶ä»»åŠ¡è®¾ç½®å®Œæˆ")
        
    def daily_sync_task(self):
        """æ¯æ—¥åŒæ­¥ä»»åŠ¡"""
        logger.info("ğŸŒ… å¼€å§‹æ¯æ—¥åŒæ­¥ä»»åŠ¡")
        
        try:
            # åŒæ­¥æ˜¨å¤©çš„æ•°æ®
            yesterday = datetime.now().strftime("%Y-%m-%d")
            
            # æŸ¥æ‰¾ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶
            data_dir = Path(self.watch_dir)
            json_files = list(data_dir.glob(f"*{yesterday}*.json"))
            
            if not json_files:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ˜¨æ—¥æ•°æ®æ–‡ä»¶: {yesterday}")
                return
                
            # æ‰¹é‡å¤„ç†æ–‡ä»¶
            for file_path in json_files:
                logger.info(f"ğŸ“ å¤„ç†æ–‡ä»¶: {file_path}")
                result = self.sync.sync_single_file(str(file_path), batch_size=50)
                
                if result["success"]:
                    logger.info(f"âœ… æ–‡ä»¶åŒæ­¥æˆåŠŸ: {result['success_count']}/{result['total_count']}")
                else:
                    logger.error(f"âŒ æ–‡ä»¶åŒæ­¥å¤±è´¥: {result.get('error')}")
                    
        except Exception as e:
            logger.error(f"ğŸ’¥ æ¯æ—¥åŒæ­¥ä»»åŠ¡å¤±è´¥: {e}")
            
    def hourly_check_task(self):
        """æ¯å°æ—¶æ£€æŸ¥ä»»åŠ¡"""
        logger.info("â° æ‰§è¡Œæ¯å°æ—¶æ£€æŸ¥")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ä»¶
            data_dir = Path(self.watch_dir)
            if not data_dir.exists():
                return
                
            # è·å–æœ€è¿‘1å°æ—¶çš„æ–‡ä»¶
            current_time = time.time()
            recent_files = []
            
            for file_path in data_dir.glob("*.json"):
                file_mtime = file_path.stat().st_mtime
                if current_time - file_mtime < 3600:  # 1å°æ—¶å†…
                    recent_files.append(file_path)
                    
            if recent_files:
                logger.info(f"ğŸ” å‘ç° {len(recent_files)} ä¸ªæ–°æ–‡ä»¶")
                for file_path in recent_files:
                    self.process_new_file(str(file_path))
            else:
                logger.info("ğŸ“­ æš‚æ— æ–°æ–‡ä»¶")
                
        except Exception as e:
            logger.error(f"ğŸ’¥ æ¯å°æ—¶æ£€æŸ¥å¤±è´¥: {e}")
            
    def cleanup_logs_task(self):
        """æ¸…ç†æ—¥å¿—ä»»åŠ¡"""
        logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†æ—§æ—¥å¿—")
        
        try:
            # æ¸…ç†7å¤©å‰çš„æ—¥å¿—
            log_files = ["scheduler.log", "feishu_sync_simple.log"]
            
            for log_file in log_files:
                if os.path.exists(log_file):
                    file_size = os.path.getsize(log_file)
                    if file_size > 10 * 1024 * 1024:  # 10MB
                        # å¤‡ä»½å¹¶é‡æ–°å¼€å§‹
                        backup_name = f"{log_file}.{int(time.time())}"
                        os.rename(log_file, backup_name)
                        logger.info(f"ğŸ“¦ æ—¥å¿—æ–‡ä»¶å·²å¤‡ä»½: {backup_name}")
                        
        except Exception as e:
            logger.error(f"ğŸ’¥ æ¸…ç†æ—¥å¿—å¤±è´¥: {e}")
            
    def process_new_file(self, file_path: str):
        """å¤„ç†æ–°æ–‡ä»¶"""
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
        
        try:
            # åŸºç¡€AIæ¸…æ´—ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
            cleaned_data = self.simple_ai_clean(file_path)
            
            if not cleaned_data:
                logger.warning(f"âš ï¸ æ–‡ä»¶æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®: {file_path}")
                return
                
            # åŒæ­¥åˆ°é£ä¹¦
            result = self.sync.sync_single_file(file_path, batch_size=30)
            
            if result["success"]:
                logger.info(f"âœ… è‡ªåŠ¨åŒæ­¥æˆåŠŸ: {result['success_count']}/{result['total_count']}")
                
                # è®°å½•å¤„ç†ç»“æœ
                self.log_sync_result(file_path, result)
            else:
                logger.error(f"âŒ è‡ªåŠ¨åŒæ­¥å¤±è´¥: {result.get('error')}")
                
        except Exception as e:
            logger.error(f"ğŸ’¥ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            
    def simple_ai_clean(self, file_path: str) -> bool:
        """ç®€å•çš„AIæ•°æ®æ¸…æ´—"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            if not data:
                return False
                
            # ç®€å•çš„è¿‡æ»¤è§„åˆ™
            original_count = len(data)
            
            # è¿‡æ»¤ç©ºå†…å®¹
            if isinstance(data, list):
                data = [item for item in data if self.is_valid_item(item)]
            
            filtered_count = len(data) if isinstance(data, list) else 1
            
            logger.info(f"ğŸ§¹ æ•°æ®æ¸…æ´—å®Œæˆ: {original_count} â†’ {filtered_count}")
            
            # å¦‚æœæ¸…æ´—åæ•°æ®é‡è¿‡å°‘ï¼Œå¯èƒ½éœ€è¦äººå·¥æ£€æŸ¥
            if filtered_count < original_count * 0.5:
                logger.warning(f"âš ï¸ æ¸…æ´—åæ•°æ®é‡å‡å°‘è¶…è¿‡50%ï¼Œè¯·æ£€æŸ¥: {file_path}")
                
            return filtered_count > 0
            
        except Exception as e:
            logger.error(f"ğŸ’¥ AIæ¸…æ´—å¤±è´¥: {e}")
            return False
            
    def is_valid_item(self, item: Dict) -> bool:
        """åˆ¤æ–­æ•°æ®é¡¹æ˜¯å¦æœ‰æ•ˆ"""
        if not isinstance(item, dict):
            return False
            
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if 'comment_id' in item:  # è¯„è®ºæ•°æ®
            required_fields = ['comment_id', 'content', 'user_id']
        else:  # ç¬”è®°æ•°æ®
            required_fields = ['note_id', 'title', 'user_id']
            
        for field in required_fields:
            if not item.get(field):
                return False
                
        # è¿‡æ»¤åƒåœ¾å†…å®¹
        content = item.get('content') or item.get('title') or item.get('desc', '')
        if not content or len(content.strip()) < 5:
            return False
            
        # è¿‡æ»¤å¸¸è§åƒåœ¾è¯
        spam_keywords = ['å¹¿å‘Š', 'åŠ å¾®ä¿¡', 'åˆ·ç²‰', 'ä»£åˆ·', 'ä¹°ç²‰']
        content_lower = content.lower()
        
        for keyword in spam_keywords:
            if keyword in content_lower:
                return False
                
        return True
        
    def log_sync_result(self, file_path: str, result: Dict):
        """è®°å½•åŒæ­¥ç»“æœ"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "file_path": file_path,
            "success_count": result.get("success_count", 0),
            "total_count": result.get("total_count", 0),
            "table_id": result.get("table_id"),
            "data_type": result.get("data_type")
        }
        
        # è¿½åŠ åˆ°åŒæ­¥æ—¥å¿—æ–‡ä»¶
        with open("sync_history.json", "a", encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
            
    def start_file_monitoring(self):
        """å¯åŠ¨æ–‡ä»¶ç›‘æ§"""
        logger.info(f"ğŸ‘€ å¼€å§‹ç›‘æ§ç›®å½•: {self.watch_dir}")
        
        # ç¡®ä¿ç›‘æ§ç›®å½•å­˜åœ¨
        Path(self.watch_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ–‡ä»¶ç›‘æ§
        event_handler = DataFileHandler(self)
        self.observer = Observer()
        self.observer.schedule(event_handler, self.watch_dir, recursive=True)
        self.observer.start()
        
        logger.info("ğŸ“ æ–‡ä»¶ç›‘æ§å·²å¯åŠ¨")
        
    def stop_file_monitoring(self):
        """åœæ­¢æ–‡ä»¶ç›‘æ§"""
        if self.observer:
            self.observer.stop()
            self.observer.join()
            logger.info("ğŸ“ æ–‡ä»¶ç›‘æ§å·²åœæ­¢")
            
    def run(self):
        """è¿è¡Œè°ƒåº¦å™¨"""
        logger.info("ğŸš€ è‡ªåŠ¨åŒ–è°ƒåº¦å™¨å¯åŠ¨")
        
        try:
            # è®¾ç½®å®šæ—¶ä»»åŠ¡
            self.setup_scheduled_tasks()
            
            # å¯åŠ¨æ–‡ä»¶ç›‘æ§
            self.start_file_monitoring()
            
            # ä¸»å¾ªç¯
            logger.info("â° è¿›å…¥è°ƒåº¦å¾ªç¯")
            while True:
                schedule.run_pending()
                time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                
        except KeyboardInterrupt:
            logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·")
        except Exception as e:
            logger.error(f"ğŸ’¥ è°ƒåº¦å™¨å¼‚å¸¸: {e}")
        finally:
            self.stop_file_monitoring()
            logger.info("ğŸ‘‹ è‡ªåŠ¨åŒ–è°ƒåº¦å™¨å·²åœæ­¢")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='MediaCrawlerè‡ªåŠ¨åŒ–è°ƒåº¦å™¨')
    parser.add_argument('--mode', choices=['daemon', 'once'], default='daemon',
                       help='è¿è¡Œæ¨¡å¼: daemon(æŒç»­è¿è¡Œ) æˆ– once(å•æ¬¡æ‰§è¡Œ)')
    parser.add_argument('--task', choices=['daily', 'hourly', 'cleanup'],
                       help='å•æ¬¡æ‰§è¡Œçš„ä»»åŠ¡ç±»å‹')
    
    args = parser.parse_args()
    
    scheduler = AutoScheduler()
    
    if args.mode == 'once':
        # å•æ¬¡æ‰§è¡Œæ¨¡å¼
        if args.task == 'daily':
            scheduler.daily_sync_task()
        elif args.task == 'hourly':
            scheduler.hourly_check_task()
        elif args.task == 'cleanup':
            scheduler.cleanup_logs_task()
        else:
            logger.error("âŒ å•æ¬¡æ¨¡å¼éœ€è¦æŒ‡å®šä»»åŠ¡ç±»å‹")
    else:
        # å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼
        scheduler.run()

if __name__ == "__main__":
    main()
