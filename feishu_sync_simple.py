#!/usr/bin/env python3
"""
å°çº¢ä¹¦æ•°æ®åŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼ - ç®€åŒ–ç‹¬ç«‹ç‰ˆæœ¬
åŸºäºé£ä¹¦å®˜æ–¹APIï¼Œä¸ä¾èµ–SDKï¼Œæ›´ç¨³å®šå¯é 

ä½¿ç”¨æ–¹æ³•ï¼š
    python feishu_sync_simple.py --file data/xhs/json/search_contents_2025-09-05.json
    python feishu_sync_simple.py --dir data/xhs/json/
    python feishu_sync_simple.py --dir data/xhs/json/ --batch-size 20
"""

import argparse
import json
import os
import sys
import time
import logging
import requests
import glob
from pathlib import Path
from typing import Dict, List, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('feishu_sync_simple.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class FeishuSimpleSync:
    """é£ä¹¦å¤šç»´è¡¨æ ¼åŒæ­¥å™¨ - ç®€åŒ–ç‹¬ç«‹ç‰ˆæœ¬"""
    
    def __init__(self, app_id: str, app_secret: str, app_token: str):
        """
        åˆå§‹åŒ–åŒæ­¥å™¨
        
        Args:
            app_id: é£ä¹¦åº”ç”¨ID
            app_secret: é£ä¹¦åº”ç”¨å¯†é’¥
            app_token: å¤šç»´è¡¨æ ¼Token
        """
        self.app_id = app_id
        self.app_secret = app_secret
        self.app_token = app_token
        self.base_url = "https://open.feishu.cn/open-apis"
        self.access_token = None
        self.token_expire_time = 0
        
    def get_access_token(self) -> str:
        """è·å–è®¿é—®ä»¤ç‰Œ"""
        if self.access_token and time.time() < self.token_expire_time:
            return self.access_token
            
        url = f"{self.base_url}/auth/v3/tenant_access_token/internal"
        payload = {
            "app_id": self.app_id,
            "app_secret": self.app_secret
        }
        
        try:
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if data.get("code") == 0:
                self.access_token = data["tenant_access_token"]
                self.token_expire_time = time.time() + data.get("expire", 7200) - 60
                logger.info("âœ… è®¿é—®ä»¤ç‰Œè·å–æˆåŠŸ")
                return self.access_token
            else:
                raise Exception(f"è·å–è®¿é—®ä»¤ç‰Œå¤±è´¥: {data}")
                
        except Exception as e:
            logger.error(f"âŒ è·å–è®¿é—®ä»¤ç‰Œå¤±è´¥: {e}")
            raise
    
    def create_table(self, table_name: str, data_type: str = "note") -> str:
        """
        åˆ›å»ºæ•°æ®è¡¨
        
        Args:
            table_name: è¡¨æ ¼åç§°
            data_type: æ•°æ®ç±»å‹ï¼Œ"note" æˆ– "comment"
        """
        access_token = self.get_access_token()
        
        # ä¸ºé¿å…é‡å¤ï¼Œæ·»åŠ æ—¶é—´æˆ³
        unique_table_name = f"{table_name}_{int(time.time())}"
        
        url = f"{self.base_url}/bitable/v1/apps/{self.app_token}/tables"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
        
        # æ ¹æ®æ•°æ®ç±»å‹è·å–å­—æ®µå®šä¹‰
        from feishu_sync.data_formatter import XHSDataFormatter
        fields = XHSDataFormatter.get_table_fields(data_type)
        
        payload = {
            "table": {
                "name": unique_table_name,
                "default_view_name": "é»˜è®¤è§†å›¾",
                "fields": fields
            }
        }
        
        try:
            logger.info(f"ğŸ“Š åˆ›å»ºæ•°æ®è¡¨: {unique_table_name}")
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if data.get("code") == 0:
                table_id = data["data"]["table_id"]
                logger.info(f"âœ… æ•°æ®è¡¨åˆ›å»ºæˆåŠŸ: {table_id}")
                return table_id
            else:
                raise Exception(f"åˆ›å»ºè¡¨æ ¼å¤±è´¥: {data}")
                
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¡¨æ ¼å¤±è´¥: {e}")
            raise
    
    def batch_upload_records(self, table_id: str, records: List[Dict], batch_size: int = 50) -> Dict:
        """æ‰¹é‡ä¸Šä¼ è®°å½•"""
        access_token = self.get_access_token()
        
        url = f"{self.base_url}/bitable/v1/apps/{self.app_token}/tables/{table_id}/records/batch_create"
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
        
        total_success = 0
        total_batches = (len(records) + batch_size - 1) // batch_size
        
        logger.info(f"ğŸ“¤ å¼€å§‹æ‰¹é‡ä¸Šä¼ : {len(records)} æ¡è®°å½•, åˆ† {total_batches} æ‰¹æ¬¡")
        
        for i in range(0, len(records), batch_size):
            batch_records = records[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            payload = {"records": batch_records}
            
            try:
                logger.info(f"â³ å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹: {len(batch_records)} æ¡è®°å½•")
                
                response = requests.post(url, headers=headers, json=payload, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                if data.get("code") == 0:
                    batch_success = len(data.get("data", {}).get("records", []))
                    total_success += batch_success
                    logger.info(f"âœ… ç¬¬ {batch_num} æ‰¹æˆåŠŸ: {batch_success} æ¡")
                else:
                    logger.error(f"âŒ ç¬¬ {batch_num} æ‰¹å¤±è´¥: {data}")
                
                # é¿å…APIé™æµ
                if i + batch_size < len(records):
                    time.sleep(0.5)
                    
            except Exception as e:
                logger.error(f"âŒ ç¬¬ {batch_num} æ‰¹å¤„ç†å¤±è´¥: {e}")
        
        return {"success": total_success, "total": len(records)}
    
    def format_xhs_data(self, raw_data: List[Dict]) -> List[Dict]:
        """æ ¼å¼åŒ–å°çº¢ä¹¦æ•°æ® - ä½¿ç”¨æ–°çš„æ•°æ®æ ¼å¼åŒ–å™¨"""
        from feishu_sync.data_formatter import XHSDataFormatter
        
        formatter = XHSDataFormatter()
        return formatter.format_batch_records(raw_data)
    
    def load_json_file(self, file_path: str) -> List[Dict]:
        """åŠ è½½JSONæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, dict):
                data = [data]
            
            logger.info(f"ğŸ“„ åŠ è½½JSONæ–‡ä»¶: {len(data)} æ¡è®°å½• - {file_path}")
            return data
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def sync_single_file(self, file_path: str, batch_size: int = 50) -> Dict:
        """åŒæ­¥å•ä¸ªæ–‡ä»¶"""
        logger.info(f"ğŸš€ å¼€å§‹åŒæ­¥æ–‡ä»¶: {file_path}")
        
        # ç”Ÿæˆè¡¨å
        file_name = os.path.basename(file_path).replace('.json', '')
        table_name = f"å°çº¢ä¹¦_{file_name}"
        
        try:
            # 1. åŠ è½½æ•°æ®
            raw_data = self.load_json_file(file_path)
            if not raw_data:
                return {"success": False, "error": "æ— æ³•åŠ è½½æ•°æ®"}
            
            # 2. è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹
            from feishu_sync.data_formatter import XHSDataFormatter
            data_type = XHSDataFormatter.detect_data_type(raw_data)
            logger.info(f"ğŸ“Š æ£€æµ‹åˆ°æ•°æ®ç±»å‹: {data_type}")
            
            # 3. æ ¼å¼åŒ–æ•°æ®
            formatted_records = self.format_xhs_data(raw_data)
            if not formatted_records:
                return {"success": False, "error": "æ²¡æœ‰æœ‰æ•ˆæ•°æ®"}
            
            # 4. åˆ›å»ºè¡¨æ ¼ï¼ˆä¼ å…¥æ•°æ®ç±»å‹ï¼‰
            table_id = self.create_table(table_name, data_type)
            
            # 5. ä¸Šä¼ æ•°æ®
            result = self.batch_upload_records(table_id, formatted_records, batch_size)
            
            # 6. ç”Ÿæˆç»“æœ
            success_count = result["success"]
            total_count = result["total"]
            
            logger.info(f"ğŸ‰ åŒæ­¥å®Œæˆ: {success_count}/{total_count} æ¡è®°å½•")
            logger.info(f"ğŸ”— è¡¨æ ¼é“¾æ¥: https://feishu.cn/base/{self.app_token}?table={table_id}")
            
            return {
                "success": True,
                "file": file_path,
                "table_name": table_name,
                "table_id": table_id,
                "success_count": success_count,
                "total_count": total_count,
                "table_url": f"https://feishu.cn/base/{self.app_token}?table={table_id}",
                "data_type": data_type
            }
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥æ–‡ä»¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def sync_directory(self, dir_path: str, pattern: str = "*.json", batch_size: int = 50) -> Dict:
        """åŒæ­¥æ•´ä¸ªç›®å½•"""
        logger.info(f"ğŸ“‚ å¼€å§‹åŒæ­¥ç›®å½•: {dir_path}")
        
        # æŸ¥æ‰¾æ–‡ä»¶
        search_pattern = os.path.join(dir_path, pattern)
        files = glob.glob(search_pattern)
        
        if not files:
            logger.warning(f"âš ï¸ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶: {search_pattern}")
            return {"success": False, "error": "æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶"}
        
        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
        
        results = []
        total_success = 0
        total_records = 0
        
        for file_path in files:
            logger.info(f"\n{'='*60}")
            result = self.sync_single_file(file_path, batch_size)
            results.append(result)
            
            if result["success"]:
                total_success += result["success_count"]
                total_records += result["total_count"]
        
        # æ±‡æ€»ç»“æœ
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¯ ç›®å½•åŒæ­¥å®Œæˆ!")
        logger.info(f"ğŸ“Š æ–‡ä»¶æ•°é‡: {len(files)}")
        logger.info(f"ğŸ“ˆ æˆåŠŸè®°å½•: {total_success}/{total_records}")
        
        successful_files = [r for r in results if r["success"]]
        for result in successful_files:
            logger.info(f"âœ… {os.path.basename(result['file'])}: {result['success_count']} æ¡ -> {result['table_url']}")
        
        return {
            "success": True,
            "total_files": len(files),
            "successful_files": len(successful_files),
            "total_success": total_success,
            "total_records": total_records,
            "results": results
        }

def load_config_from_env() -> Dict:
    """ä»ç¯å¢ƒå˜é‡æˆ–.envæ–‡ä»¶åŠ è½½é…ç½®"""
    config = {}
    
    # å°è¯•åŠ è½½.envæ–‡ä»¶
    env_file = ".env"
    if os.path.exists(env_file):
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()
    
    # è¯»å–å¿…è¦é…ç½®
    config['app_id'] = os.getenv('FEISHU_APP_ID', '')
    config['app_secret'] = os.getenv('FEISHU_APP_SECRET', '')
    config['app_token'] = os.getenv('FEISHU_APP_TOKEN', '')
    
    # éªŒè¯é…ç½®
    missing = [k for k, v in config.items() if not v]
    if missing:
        raise ValueError(f"ç¼ºå°‘å¿…è¦é…ç½®: {missing}")
    
    return config

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å°çº¢ä¹¦æ•°æ®åŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼ - ç®€åŒ–ç‰ˆ')
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--file', help='åŒæ­¥å•ä¸ªJSONæ–‡ä»¶')
    group.add_argument('--dir', help='åŒæ­¥ç›®å½•ä¸‹çš„æ‰€æœ‰JSONæ–‡ä»¶')
    
    parser.add_argument('--pattern', default='*.json', help='æ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: *.json)')
    parser.add_argument('--batch-size', type=int, default=50, help='æ‰¹é‡ä¸Šä¼ å¤§å° (é»˜è®¤: 50)')
    parser.add_argument('--config', action='store_true', help='æ˜¾ç¤ºå½“å‰é…ç½®')
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½é…ç½®
        config = load_config_from_env()
        
        if args.config:
            logger.info("ğŸ“‹ å½“å‰é…ç½®:")
            logger.info(f"  APP_ID: {config['app_id'][:10]}...")
            logger.info(f"  APP_SECRET: {config['app_secret'][:10]}...")
            logger.info(f"  APP_TOKEN: {config['app_token'][:10]}...")
            return
        
        # åˆ›å»ºåŒæ­¥å™¨
        sync = FeishuSimpleSync(
            app_id=config['app_id'],
            app_secret=config['app_secret'],
            app_token=config['app_token']
        )
        
        # æ‰§è¡ŒåŒæ­¥
        if args.file:
            if not os.path.exists(args.file):
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
                sys.exit(1)
            
            result = sync.sync_single_file(args.file, args.batch_size)
            
            if not result["success"]:
                logger.error(f"âŒ åŒæ­¥å¤±è´¥: {result.get('error')}")
                sys.exit(1)
        
        elif args.dir:
            if not os.path.exists(args.dir):
                logger.error(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.dir}")
                sys.exit(1)
            
            result = sync.sync_directory(args.dir, args.pattern, args.batch_size)
            
            if not result["success"]:
                logger.error(f"âŒ ç›®å½•åŒæ­¥å¤±è´¥: {result.get('error')}")
                sys.exit(1)
        
        logger.info("ğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"ğŸ’¥ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
